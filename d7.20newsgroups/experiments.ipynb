{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86bf8f8-00be-4eab-aec7-9cd1f8b7d9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import pandas as pd\n",
    "from neo4j import GraphDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023b019d-b62b-4edd-a1d8-2b31869ae739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection setup\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"eigen1234\"\n",
    "database=\"d6.20newsgroups\"\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cb0ada-8f07-49b4-bd82-7c0a08a2a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_connection():\n",
    "    try:\n",
    "        # Establish a session with the specified database\n",
    "        # with driver.session(database=database_name) as session:\n",
    "        with driver.session() as session:\n",
    "            # Run a simple query to check the connection\n",
    "            result = session.run(\"RETURN 'Connection to database successful' AS message\")\n",
    "            for record in result:\n",
    "                print(record[\"message\"])\n",
    "    except Exception as e:\n",
    "        print(\"Error connecting to the database:\", e)\n",
    "\n",
    "# Call the check_connection function\n",
    "check_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98845cb5-edd2-4f60-a058-c5681a1e28f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run a query and measure performance metrics\n",
    "def run_query(driver, query, parameters):\n",
    "    # Start time and resources\n",
    "    process = psutil.Process(os.getpid())\n",
    "    start_time = time.time()\n",
    "    start_cpu_times = process.cpu_times()\n",
    "    start_mem = process.memory_info().rss  # Resident Set Size\n",
    "\n",
    "    with driver.session(database=database_name) as session:\n",
    "        result = session.run(query, parameters)\n",
    "        record = result.single()\n",
    "        data = record.data() if record else None\n",
    "\n",
    "    # End time and resources\n",
    "    end_time = time.time()\n",
    "    end_cpu_times = process.cpu_times()\n",
    "    end_mem = process.memory_info().rss\n",
    "\n",
    "    # Calculations\n",
    "    duration = end_time - start_time\n",
    "    cpu_used = (end_cpu_times.user + end_cpu_times.system) - (start_cpu_times.user + start_cpu_times.system)\n",
    "    memory_used = (end_mem - start_mem) / (1024 ** 2)  # Convert to MB\n",
    "\n",
    "    return data, duration, memory_used, cpu_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d745d49-6842-4092-b98c-bfe209003309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined experiments configurations with all combinations for each sub-dataset\n",
    "experiments = []\n",
    "\n",
    "# Define graph types and Laplacian methods\n",
    "graph_types = [\"full\", \"eps\", \"knn\", \"mknn\"]\n",
    "laplacian_types = [\"sym\", \"rw\", \"ad\"]\n",
    "\n",
    "# Define parameters for each graph type per sub-dataset\n",
    "parameters = {\n",
    "    \"newsgroups_33\": {\"full\": \"9\", \"eps\": \"1.141\", \"knn\": \"20\", \"mknn\": \"24\"}\n",
    "}\n",
    "\n",
    "# Define number of eigenvectors and silhouette usage (common for all experiments)\n",
    "number_of_eigenvectors = 3\n",
    "use_kmean_for_silhouette = False\n",
    "\n",
    "# Generate experiment configurations for each sub-dataset\n",
    "sub_datasets = [\"newsgroups_33\"]\n",
    "for sub_dataset in sub_datasets:\n",
    "    for graph_type in graph_types:\n",
    "        for laplacian_type in laplacian_types:\n",
    "            experiments.append({\n",
    "                \"node_label\": sub_dataset,\n",
    "                \"is_feature_based\": True,\n",
    "                \"graph_type\": graph_type,\n",
    "                \"parameter\": parameters[sub_dataset][graph_type],\n",
    "                \"laplacian_type\": laplacian_type,\n",
    "                \"number_of_eigenvectors\": number_of_eigenvectors,\n",
    "                \"use_kmean_for_silhouette\": use_kmean_for_silhouette\n",
    "            })\n",
    "\n",
    "# Print or analyze the configurations to ensure correctness\n",
    "# for experiment in experiments:\n",
    "#     print(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84087cd7-5605-42ae-91e6-b30625a3e4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(driver):\n",
    "    results = []\n",
    "    total_experiments = len(experiments)\n",
    "\n",
    "    for idx, config in enumerate(experiments, 1):\n",
    "        query = \"\"\"\n",
    "        WITH simkit.experimental_spectralClustering({\n",
    "            node_label: $node_label,\n",
    "            is_feature_based: $is_feature_based,\n",
    "            distance_measure: \"euclidean\",\n",
    "            graph_type: $graph_type,\n",
    "            parameter: $parameter,\n",
    "            remove_columns: \"Index,target\",\n",
    "            laplacian_type: $laplacian_type,\n",
    "            number_of_eigenvectors: $number_of_eigenvectors,\n",
    "            number_of_iterations: \"100\",\n",
    "            distance_measure_kmean: \"euclidean\",\n",
    "            target_column: \"target\",\n",
    "            use_kmean_for_silhouette: $use_kmean_for_silhouette,\n",
    "            seed: 42\n",
    "        }) AS result\n",
    "        RETURN result.silhouette_score AS silhouette_score, \n",
    "               result.rand_index AS rand_index,\n",
    "               result.total_time AS total_time,\n",
    "               result.affinity_time AS affinity_time,\n",
    "               result.laplacian_time AS laplacian_time,\n",
    "               result.clustering_time AS clustering_time,\n",
    "               result.adjusted_rand_index_time AS adjusted_rand_index_time\n",
    "        \"\"\"\n",
    "\n",
    "        # Measure Memory and CPU Usage Before Query Execution\n",
    "        memory_before = psutil.virtual_memory().used\n",
    "        cpu_before = psutil.cpu_percent(interval=None)\n",
    "\n",
    "        # Execute the Query and Measure Time\n",
    "        start_time = time.perf_counter()\n",
    "        data, _, _, _ = run_query(driver, query, config)\n",
    "        elapsed_time = time.perf_counter() - start_time\n",
    "\n",
    "        # Measure Memory and CPU Usage After Query Execution\n",
    "        memory_after = psutil.virtual_memory().used\n",
    "        cpu_after = psutil.cpu_percent(interval=None)\n",
    "\n",
    "        # Calculate Metrics\n",
    "        memory_used = (memory_after - memory_before) / 1e6  # Memory in MB\n",
    "        cpu_used = (cpu_before + cpu_after) / 2  # Average CPU usage\n",
    "\n",
    "        # Extract Results from Java\n",
    "        silhouette_score = data['silhouette_score'] if data else None\n",
    "        rand_index = data['rand_index'] if data else None\n",
    "        total_time = data['total_time'] if data else None\n",
    "        affinity_time = data['affinity_time'] if data else None\n",
    "        laplacian_time = data['laplacian_time'] if data else None\n",
    "        clustering_time = data['clustering_time'] if data else None\n",
    "        adjusted_rand_index_time = data['adjusted_rand_index_time'] if data else None\n",
    "\n",
    "        # Save Results\n",
    "        results.append({\n",
    "            **config,\n",
    "            \"silhouette_score\": silhouette_score,\n",
    "            \"rand_index\": rand_index,\n",
    "            \"total_time\": total_time or elapsed_time,\n",
    "            \"affinity_time\": affinity_time,\n",
    "            \"laplacian_time\": laplacian_time,\n",
    "            \"clustering_time\": clustering_time,\n",
    "            \"adjusted_rand_index_time\": adjusted_rand_index_time,\n",
    "            \"memory_used\": memory_used,\n",
    "            \"cpu_used\": cpu_used\n",
    "        })\n",
    "\n",
    "        print(f\"Completed experiment {idx}/{total_experiments} with config: {config}\")\n",
    "\n",
    "    driver.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99eb9d4-938b-4e4b-b7c7-1dd07d18b113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example scheduling and execution\n",
    "# def job():\n",
    "#     print(\"Running experiments...\")\n",
    "#     result_data = run_experiments(driver)\n",
    "#     # Save to DataFrame and then to CSV\n",
    "#     df = pd.DataFrame(result_data)\n",
    "#     df.to_csv(\"iris_results.csv\", index=False)\n",
    "#     print(\"Experiments completed and saved.\")\n",
    "\n",
    "# schedule.every().day.at(\"01:00\").do(job)\n",
    "\n",
    "# # Loop to keep the scheduler running\n",
    "# while True:\n",
    "#     schedule.run_pending()\n",
    "#     time.sleep(60)  # wait one minute\n",
    "\n",
    "print(\"Running experiments...\")\n",
    "result_data1 = run_experiments(driver)\n",
    "# Save to DataFrame and then to CSV\n",
    "df1 = pd.DataFrame(result_data1)\n",
    "df1.to_csv(\"newsgroups_results1.csv\", index=False)\n",
    "print(\"Experiments completed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e8d25a-ac59-45ed-9bb9-814d6cc97fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab1798f-fd86-4910-9242-ba3d24c66581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined experiments configurations with all combinations for each sub-dataset\n",
    "experiments = []\n",
    "\n",
    "# Define graph types and Laplacian methods\n",
    "graph_types = [\"full\", \"eps\", \"knn\", \"mknn\"]\n",
    "laplacian_types = [\"sym\", \"rw\", \"ad\"]\n",
    "\n",
    "# Define parameters for each graph type per sub-dataset\n",
    "parameters = {\n",
    "    \"newsgroups_66\": {\"full\": \"5\", \"eps\": \"1.143\", \"knn\": \"14\", \"mknn\": \"16\"}\n",
    "}\n",
    "\n",
    "# Define number of eigenvectors and silhouette usage (common for all experiments)\n",
    "number_of_eigenvectors = 3\n",
    "use_kmean_for_silhouette = False\n",
    "\n",
    "# Generate experiment configurations for each sub-dataset\n",
    "sub_datasets = [\"newsgroups_66\"]\n",
    "for sub_dataset in sub_datasets:\n",
    "    for graph_type in graph_types:\n",
    "        for laplacian_type in laplacian_types:\n",
    "            experiments.append({\n",
    "                \"node_label\": sub_dataset,\n",
    "                \"is_feature_based\": True,\n",
    "                \"graph_type\": graph_type,\n",
    "                \"parameter\": parameters[sub_dataset][graph_type],\n",
    "                \"laplacian_type\": laplacian_type,\n",
    "                \"number_of_eigenvectors\": number_of_eigenvectors,\n",
    "                \"use_kmean_for_silhouette\": use_kmean_for_silhouette\n",
    "            })\n",
    "\n",
    "# Print or analyze the configurations to ensure correctness\n",
    "# for experiment in experiments:\n",
    "#     print(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7feb3d-9e33-4f19-9c8c-b7fa93b832de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(driver):\n",
    "    results = []\n",
    "    total_experiments = len(experiments)\n",
    "\n",
    "    for idx, config in enumerate(experiments, 1):\n",
    "        query = \"\"\"\n",
    "        WITH simkit.experimental_spectralClustering({\n",
    "            node_label: $node_label,\n",
    "            is_feature_based: $is_feature_based,\n",
    "            distance_measure: \"euclidean\",\n",
    "            graph_type: $graph_type,\n",
    "            parameter: $parameter,\n",
    "            remove_columns: \"Index,target\",\n",
    "            laplacian_type: $laplacian_type,\n",
    "            number_of_eigenvectors: $number_of_eigenvectors,\n",
    "            number_of_iterations: \"100\",\n",
    "            distance_measure_kmean: \"euclidean\",\n",
    "            target_column: \"target\",\n",
    "            use_kmean_for_silhouette: $use_kmean_for_silhouette,\n",
    "            seed: 42\n",
    "        }) AS result\n",
    "        RETURN result.silhouette_score AS silhouette_score, \n",
    "               result.rand_index AS rand_index,\n",
    "               result.total_time AS total_time,\n",
    "               result.affinity_time AS affinity_time,\n",
    "               result.laplacian_time AS laplacian_time,\n",
    "               result.clustering_time AS clustering_time,\n",
    "               result.adjusted_rand_index_time AS adjusted_rand_index_time\n",
    "        \"\"\"\n",
    "\n",
    "        # Measure Memory and CPU Usage Before Query Execution\n",
    "        memory_before = psutil.virtual_memory().used\n",
    "        cpu_before = psutil.cpu_percent(interval=None)\n",
    "\n",
    "        # Execute the Query and Measure Time\n",
    "        start_time = time.perf_counter()\n",
    "        data, _, _, _ = run_query(driver, query, config)\n",
    "        elapsed_time = time.perf_counter() - start_time\n",
    "\n",
    "        # Measure Memory and CPU Usage After Query Execution\n",
    "        memory_after = psutil.virtual_memory().used\n",
    "        cpu_after = psutil.cpu_percent(interval=None)\n",
    "\n",
    "        # Calculate Metrics\n",
    "        memory_used = (memory_after - memory_before) / 1e6  # Memory in MB\n",
    "        cpu_used = (cpu_before + cpu_after) / 2  # Average CPU usage\n",
    "\n",
    "        # Extract Results from Java\n",
    "        silhouette_score = data['silhouette_score'] if data else None\n",
    "        rand_index = data['rand_index'] if data else None\n",
    "        total_time = data['total_time'] if data else None\n",
    "        affinity_time = data['affinity_time'] if data else None\n",
    "        laplacian_time = data['laplacian_time'] if data else None\n",
    "        clustering_time = data['clustering_time'] if data else None\n",
    "        adjusted_rand_index_time = data['adjusted_rand_index_time'] if data else None\n",
    "\n",
    "        # Save Results\n",
    "        results.append({\n",
    "            **config,\n",
    "            \"silhouette_score\": silhouette_score,\n",
    "            \"rand_index\": rand_index,\n",
    "            \"total_time\": total_time or elapsed_time,\n",
    "            \"affinity_time\": affinity_time,\n",
    "            \"laplacian_time\": laplacian_time,\n",
    "            \"clustering_time\": clustering_time,\n",
    "            \"adjusted_rand_index_time\": adjusted_rand_index_time,\n",
    "            \"memory_used\": memory_used,\n",
    "            \"cpu_used\": cpu_used\n",
    "        })\n",
    "\n",
    "        print(f\"Completed experiment {idx}/{total_experiments} with config: {config}\")\n",
    "\n",
    "    driver.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa20191c-fc85-45b2-9a1c-d879b80e9d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example scheduling and execution\n",
    "# def job():\n",
    "#     print(\"Running experiments...\")\n",
    "#     result_data = run_experiments(driver)\n",
    "#     # Save to DataFrame and then to CSV\n",
    "#     df = pd.DataFrame(result_data)\n",
    "#     df.to_csv(\"iris_results.csv\", index=False)\n",
    "#     print(\"Experiments completed and saved.\")\n",
    "\n",
    "# schedule.every().day.at(\"01:00\").do(job)\n",
    "\n",
    "# # Loop to keep the scheduler running\n",
    "# while True:\n",
    "#     schedule.run_pending()\n",
    "#     time.sleep(60)  # wait one minute\n",
    "\n",
    "print(\"Running experiments...\")\n",
    "result_data2 = run_experiments(driver)\n",
    "# Save to DataFrame and then to CSV\n",
    "df2 = pd.DataFrame(result_data2)\n",
    "df2.to_csv(\"newsgroups_results2.csv\", index=False)\n",
    "print(\"Experiments completed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83f4b26-2cd2-48b9-b28a-0cbf338c1f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b137b9d-16fd-4bdb-a8a5-f899610d1a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined experiments configurations with all combinations for each sub-dataset\n",
    "experiments = []\n",
    "\n",
    "# Define graph types and Laplacian methods\n",
    "graph_types = [\"full\", \"eps\", \"knn\", \"mknn\"]\n",
    "laplacian_types = [\"sym\", \"rw\", \"ad\"]\n",
    "\n",
    "# Define parameters for each graph type per sub-dataset\n",
    "parameters = {\n",
    "    \"newsgroups_full\": {\"full\": \"11\", \"eps\": \"1.161\", \"knn\": \"28\", \"mknn\": \"29\"}\n",
    "}\n",
    "\n",
    "# Define number of eigenvectors and silhouette usage (common for all experiments)\n",
    "number_of_eigenvectors = 3\n",
    "use_kmean_for_silhouette = False\n",
    "\n",
    "# Generate experiment configurations for each sub-dataset\n",
    "sub_datasets = [\"newsgroups_full\"]\n",
    "for sub_dataset in sub_datasets:\n",
    "    for graph_type in graph_types:\n",
    "        for laplacian_type in laplacian_types:\n",
    "            experiments.append({\n",
    "                \"node_label\": sub_dataset,\n",
    "                \"is_feature_based\": True,\n",
    "                \"graph_type\": graph_type,\n",
    "                \"parameter\": parameters[sub_dataset][graph_type],\n",
    "                \"laplacian_type\": laplacian_type,\n",
    "                \"number_of_eigenvectors\": number_of_eigenvectors,\n",
    "                \"use_kmean_for_silhouette\": use_kmean_for_silhouette\n",
    "            })\n",
    "\n",
    "# Print or analyze the configurations to ensure correctness\n",
    "# for experiment in experiments:\n",
    "#     print(experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ec7924-f5df-473e-9034-687810b259b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(driver):\n",
    "    results = []\n",
    "    total_experiments = len(experiments)\n",
    "\n",
    "    for idx, config in enumerate(experiments, 1):\n",
    "        query = \"\"\"\n",
    "        WITH simkit.experimental_spectralClustering({\n",
    "            node_label: $node_label,\n",
    "            is_feature_based: $is_feature_based,\n",
    "            distance_measure: \"euclidean\",\n",
    "            graph_type: $graph_type,\n",
    "            parameter: $parameter,\n",
    "            remove_columns: \"Index,target\",\n",
    "            laplacian_type: $laplacian_type,\n",
    "            number_of_eigenvectors: $number_of_eigenvectors,\n",
    "            number_of_iterations: \"100\",\n",
    "            distance_measure_kmean: \"euclidean\",\n",
    "            target_column: \"target\",\n",
    "            use_kmean_for_silhouette: $use_kmean_for_silhouette,\n",
    "            seed: 42\n",
    "        }) AS result\n",
    "        RETURN result.silhouette_score AS silhouette_score, \n",
    "               result.rand_index AS rand_index,\n",
    "               result.total_time AS total_time,\n",
    "               result.affinity_time AS affinity_time,\n",
    "               result.laplacian_time AS laplacian_time,\n",
    "               result.clustering_time AS clustering_time,\n",
    "               result.adjusted_rand_index_time AS adjusted_rand_index_time\n",
    "        \"\"\"\n",
    "\n",
    "        # Measure Memory and CPU Usage Before Query Execution\n",
    "        memory_before = psutil.virtual_memory().used\n",
    "        cpu_before = psutil.cpu_percent(interval=None)\n",
    "\n",
    "        # Execute the Query and Measure Time\n",
    "        start_time = time.perf_counter()\n",
    "        data, _, _, _ = run_query(driver, query, config)\n",
    "        elapsed_time = time.perf_counter() - start_time\n",
    "\n",
    "        # Measure Memory and CPU Usage After Query Execution\n",
    "        memory_after = psutil.virtual_memory().used\n",
    "        cpu_after = psutil.cpu_percent(interval=None)\n",
    "\n",
    "        # Calculate Metrics\n",
    "        memory_used = (memory_after - memory_before) / 1e6  # Memory in MB\n",
    "        cpu_used = (cpu_before + cpu_after) / 2  # Average CPU usage\n",
    "\n",
    "        # Extract Results from Java\n",
    "        silhouette_score = data['silhouette_score'] if data else None\n",
    "        rand_index = data['rand_index'] if data else None\n",
    "        total_time = data['total_time'] if data else None\n",
    "        affinity_time = data['affinity_time'] if data else None\n",
    "        laplacian_time = data['laplacian_time'] if data else None\n",
    "        clustering_time = data['clustering_time'] if data else None\n",
    "        adjusted_rand_index_time = data['adjusted_rand_index_time'] if data else None\n",
    "\n",
    "        # Save Results\n",
    "        results.append({\n",
    "            **config,\n",
    "            \"silhouette_score\": silhouette_score,\n",
    "            \"rand_index\": rand_index,\n",
    "            \"total_time\": total_time or elapsed_time,\n",
    "            \"affinity_time\": affinity_time,\n",
    "            \"laplacian_time\": laplacian_time,\n",
    "            \"clustering_time\": clustering_time,\n",
    "            \"adjusted_rand_index_time\": adjusted_rand_index_time,\n",
    "            \"memory_used\": memory_used,\n",
    "            \"cpu_used\": cpu_used\n",
    "        })\n",
    "\n",
    "        print(f\"Completed experiment {idx}/{total_experiments} with config: {config}\")\n",
    "\n",
    "    driver.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e16f915-a9f1-40bf-ab2f-4f085d221088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example scheduling and execution\n",
    "# def job():\n",
    "#     print(\"Running experiments...\")\n",
    "#     result_data = run_experiments(driver)\n",
    "#     # Save to DataFrame and then to CSV\n",
    "#     df = pd.DataFrame(result_data)\n",
    "#     df.to_csv(\"iris_results.csv\", index=False)\n",
    "#     print(\"Experiments completed and saved.\")\n",
    "\n",
    "# schedule.every().day.at(\"01:00\").do(job)\n",
    "\n",
    "# # Loop to keep the scheduler running\n",
    "# while True:\n",
    "#     schedule.run_pending()\n",
    "#     time.sleep(60)  # wait one minute\n",
    "\n",
    "print(\"Running experiments...\")\n",
    "result_data3 = run_experiments(driver)\n",
    "# Save to DataFrame and then to CSV\n",
    "df3 = pd.DataFrame(result_data3)\n",
    "df3.to_csv(\"newsgroups_results3.csv\", index=False)\n",
    "print(\"Experiments completed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91824d3b-81e2-4762-86a5-147b48d14501",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
